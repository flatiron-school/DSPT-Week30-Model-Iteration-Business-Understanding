{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Iteration and Business Understanding \n",
    "\n",
    "## Through the lens of Product Review Classification\n",
    "\n",
    "**Unlike previous challenges, we have provided you with a notebook containing some pre-existing code.**  Your work should be done in this notebook, with your code and markdown integrated into the existing code and markdown, starting with the markdown header that says \"Your Turn\" down below. Be sure to run all the steps up to that point, and read their findings!\n",
    "\n",
    "Make sure that your code is clean and readable, and that each step of your process is documented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "Northwind Trading Company allows customers to leave reviews, but those reviews do not have customer-facing \"star ratings\".  Instead, customers are free to write text, and other customers can vote on whether the review was helpful.  They find that this is a good trade-off between helping customers make informed decisions about products, and avoiding having any products go unsold because of poor ratings.\n",
    "\n",
    "Internally, Northwind is interested to know which of these reviews are positive, and which are negative.  **A previous employee of the company has already built a Random Forest Classifier model to perform this classification task.**\n",
    "\n",
    "Northwind management has heard great things about using Artificial Intelligence for this kind of task, especially Neural Networks built with TensorFlow.  \n",
    "\n",
    "### You have been instructed to build a TensorFlow model and advise the company on whether they should switch from the Random Forest Classifier to the TensorFlow model.\n",
    "\n",
    "In either case, you want a **classification model** that optimizes for **accuracy**.\n",
    "\n",
    "## Data Understanding\n",
    "\n",
    "We have a labeled collection of 20,000 product reviews, with an equal split of positive and negative reviews. The dataset contains the following features:\n",
    "\n",
    " - `ProductId` Unique identifier for the product\n",
    " - `UserId` Unqiue identifier for the user\n",
    " - `ProfileName` Profile name of the user\n",
    " - `HelpfulnessNumerator` Number of users who found the review helpful\n",
    " - `HelpfulnessDenominator` Number of users who indicated whether they found the review helpful or not\n",
    " - `Time` Timestamp for the review\n",
    " - `Summary` Brief summary of the review\n",
    " - `Text` Text of the review\n",
    " - `PositiveReview` 1 if this was labeled as a positive review, 0 if it was labeled as a negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>PositiveReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B002QWHJOU</td>\n",
       "      <td>A37565LZHTG1VH</td>\n",
       "      <td>C. Maltese</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1305331200</td>\n",
       "      <td>Awesome!</td>\n",
       "      <td>This is a great product. My 2 year old Golden ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000ESLJ6C</td>\n",
       "      <td>AMUAWXDJHE4D2</td>\n",
       "      <td>angieseashore</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1320710400</td>\n",
       "      <td>Was there a recipe change?</td>\n",
       "      <td>I have been drinking Pero ever since I was a l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B004IJJQK4</td>\n",
       "      <td>AMHHNAFJ9L958</td>\n",
       "      <td>A M</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1321747200</td>\n",
       "      <td>These taste so bland.</td>\n",
       "      <td>Look, each pack contains two servings of 120 c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId          UserId    ProfileName  HelpfulnessNumerator  \\\n",
       "0  B002QWHJOU  A37565LZHTG1VH     C. Maltese                     1   \n",
       "1  B000ESLJ6C   AMUAWXDJHE4D2  angieseashore                     1   \n",
       "2  B004IJJQK4   AMHHNAFJ9L958            A M                     0   \n",
       "\n",
       "   HelpfulnessDenominator        Time                     Summary  \\\n",
       "0                       1  1305331200                    Awesome!   \n",
       "1                       1  1320710400  Was there a recipe change?   \n",
       "2                       1  1321747200       These taste so bland.   \n",
       "\n",
       "                                                Text  PositiveReview  \n",
       "0  This is a great product. My 2 year old Golden ...               1  \n",
       "1  I have been drinking Pero ever since I was a l...               0  \n",
       "2  Look, each pack contains two servings of 120 c...               0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/reviews.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has already been cleaned, so there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductId                 0\n",
       "UserId                    0\n",
       "ProfileName               0\n",
       "HelpfulnessNumerator      0\n",
       "HelpfulnessDenominator    0\n",
       "Time                      0\n",
       "Summary                   0\n",
       "Text                      0\n",
       "PositiveReview            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PositiveReview` is the target, and all other columns are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"PositiveReview\", axis=1)\n",
    "y = df[\"PositiveReview\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "First, split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, prepare for modeling. The following `Pipeline` prepares all data for modeling.  It one-hot encodes the `ProductId`, applies a tf-idf vectorizer to the `Summary` and `Text`, keeps the numeric columns as-is, and drops all other columns.\n",
    "\n",
    "The following code may take up to 1 minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 11275)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_irrelevant_columns(X):\n",
    "    return X.drop([\"UserId\", \"ProfileName\"], axis=1)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"drop_columns\", FunctionTransformer(drop_irrelevant_columns)),\n",
    "    (\"transform_text_columns\", ColumnTransformer(transformers=[\n",
    "        (\"ohe\", OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\", sparse=False), [\"ProductId\"]),\n",
    "        (\"summary-tf-idf\", TfidfVectorizer(max_features=1000), \"Summary\"),\n",
    "        (\"text-tf-idf\", TfidfVectorizer(max_features=1000), \"Text\")\n",
    "    ], remainder=\"passthrough\"))\n",
    "])\n",
    "\n",
    "X_train_transformed = pipeline.fit_transform(X_train)\n",
    "X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Current Model\n",
    "\n",
    "`RandomForestClassifier` with the best hyperparameters.  The following code may take up to 1 minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=30, min_samples_split=15, random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_depth=30,\n",
    "    min_samples_split=15,\n",
    "    min_samples_leaf=1\n",
    ")\n",
    "rfc.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "We are using _accuracy_ as our metric, which is the default metric in Scikit-Learn, so it is possible to just use the built-in `.score` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9826666666666667\n",
      "Test accuracy: 0.913\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy:\", rfc.score(X_train_transformed, y_train))\n",
    "print(\"Test accuracy:\", rfc.score(X_test_transformed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train confusion matrix:\n",
      "[[7312  177]\n",
      " [  83 7428]]\n",
      "Test confusion matrix:\n",
      "[[2293  218]\n",
      " [ 217 2272]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train confusion matrix:\")\n",
    "print(confusion_matrix(y_train, rfc.predict(X_train_transformed)))\n",
    "print(\"Test confusion matrix:\")\n",
    "print(confusion_matrix(y_test, rfc.predict(X_test_transformed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Interpretation\n",
    "\n",
    "The tuned Random Forest Classifier model appears to be somewhat overfit on the training data, but nevertheless achieves 91% accuracy on the test data.  Of the 9% of mislabeled comments, about half are false positives and half are false negatives.\n",
    "\n",
    "Because this is a balanced dataset, 91% accuracy is a substantial improvement over a 50% baseline.  This model is ready for production use for decision support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "A train-test split has already been performed.\n",
    "\n",
    "Additionally, there is already a pipeline in place that drops some columns and converts all text columns into a numeric format for modeling.\n",
    "\n",
    "**Your only additional data preparation task is feature scaling.**  Tree-based models like Random Forest Classifiers do not require scaling, but TensorFlow neural networks do.\n",
    "\n",
    "There are two main strategies you can take for this task:\n",
    "\n",
    "#### Scaling within the existing pipeline\n",
    "\n",
    "If you are comfortable with pipelines, this is the more polished/professional route.\n",
    "\n",
    "1. Make a new pipeline, with a `StandardScaler` as the final step.  You can nest the steps of the previous pipeline inside of this new pipeline\n",
    "2. Generate a new `X_train_transformed_scaled` by calling `.fit_transform` on the new pipeline\n",
    "3. Generate a new `X_test_transformed_scaled` by calling `.transform` on the new pipeline\n",
    "\n",
    "#### Scaling after the pipeline has finished\n",
    "\n",
    "This is a better strategy if you are not as comfortable with pipelines.\n",
    "\n",
    "1. Instantiate a `StandardScaler` object\n",
    "2. Generate a new `X_train_transformed_scaled` by calling `.fit_transform` on the scaler object, after you have called `.fit_transform` on the pipeline\n",
    "3. Generate a new `X_test_transformed_scaled` by calling `.transform` on the scaler object, after you have called `.transform` on the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "Build a neural network classifier.  Specifically, use the `keras` submodule of the `tensorflow` library to build a multi-layer perceptron model with the `Sequential` interface.\n",
    "\n",
    "See the [`tf.keras` documentation](https://www.tensorflow.org/guide/keras/overview) for an overview on the use of `Sequential` models. See the [Keras layers documentation](https://keras.io/layers/core/) for descriptions of the `Dense` layer options.  \n",
    "\n",
    "1. Instantiate a `Sequential` model\n",
    "2. Add an input `Dense` layer.  You'll need to specify a `input_shape` = (11275,) because this is the number of features of the transformed dataset.\n",
    "3. Add one or more `Dense` hidden layers.  They can have any number of units, but keep in mind that more units will require more processing power.  We recommend an initial `units` of 64 for processing power reasons.\n",
    "4. Add a final `Dense` output layer.  This layer must have exactly 1 unit because we are doing a binary prediction task.\n",
    "5. Compile the `Sequential` model\n",
    "6. Fit the `Sequential` model on the preprocessed training data (`X_train_transformed_scaled`).  We recommend an initial `batch_size` of 50 and `epochs` of 5 for processing power reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning + Feature Engineering\n",
    "\n",
    "If you are running out of time, skip this step.\n",
    "\n",
    "Tune the neural network model to improve performance.  This could include steps such as increasing the units, changing the activation functions, or adding regularization.\n",
    "\n",
    "We recommend using using a `validation_split` of 0.1 to understand model performance without utilizing the test holdout set.\n",
    "\n",
    "You can also return to the preprocessing phase, and add additional features to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Choose a final `Sequential` model, add layers, and compile.  Fit the model on the preprocessed training data (`X_train_transformed_scaled`, `y_train`) and evaluate on the preprocessed testing data (`X_test_transformed_scaled`, `y_test`) using `accuracy_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical Communication\n",
    "\n",
    "Write a paragraph explaining whether Northwind Trading Company should switch to using your new neural network model, or continue to use the Random Forest Classifier.  Beyond a simple comparison of performance, try to take into consideration additional considerations such as:\n",
    "\n",
    " - Computational complexity/resource use\n",
    " - Anticipated performance on future datasets (how might the data change over time?)\n",
    " - Types of mistakes made by the two kinds of models\n",
    "\n",
    "You can make guesses or inferences about these considerations.\n",
    "\n",
    "**Include at least one visualization** comparing the two types of models.  Possible points of comparison could include ROC curves, colorized confusion matrices, or time needed to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYour paragraph here.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Your paragraph here.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
